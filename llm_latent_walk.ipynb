{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ! peaceful village rests by the lake.!\n",
      "Step 1: ! peaceful village rests by the lake.!\n",
      "Step 2: ! peaceful village rests by the lake.!\n",
      "Step 3: ! peaceful village rests by the lake.!\n",
      "Step 4: ! peaceful village rests by the lake.!\n",
      "Step 5: ! peaceful village rests by the lake.!\n",
      "Step 6: ! peaceful village restsows the lake.!\n",
      "Step 7: ! peaceful village restsows the lake.!\n",
      "Step 8: ! peaceful village restsows the lake.!\n",
      "Step 9: ! peaceful village restsows the lake.!\n",
      "Step 10: ! peaceful village restsows the lake.!\n",
      "Step 11: ! peaceful village restsows the lake.!\n",
      "Step 12: ! peaceful village restsows the lake lights!\n",
      "Step 13: ! peaceful village restsows the lake lights!\n",
      "Step 14: ! peaceful village restsows under neon lights!\n",
      "Step 15: ! futuristic village restsows under neon lights!\n",
      "Step 16: ! futuristic village restsows under neon lights!\n",
      "Step 17: ! futuristic village restsows under neon lights!\n",
      "Step 18: ! futuristic city restsows under neon lights!\n",
      "Step 19: ! futuristic city glows under neon lights!\n",
      "Step 20: ! futuristic city glows under neon lights!\n",
      "Step 21: ! futuristic city glows under neon lights!\n",
      "Step 22: ! futuristic city glows under neon lights!\n",
      "Step 23: ! futuristic city glows under neon lights!\n",
      "Step 24: ! futuristic city glows under neon lights!\n",
      "Step 25: ! futuristic city glows under neon lights!\n",
      "Step 26: ! futuristic city glows under neon lights!\n",
      "Step 27: ! futuristic city glows under neon lights!\n",
      "Step 28: ! futuristic city glows under neon lights!\n",
      "Step 29: ! futuristic city glows under neon lights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52636/3160894687.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  dot = np.sum(v0 * v1, axis=-1) / (np.linalg.norm(v0, axis=-1) * np.linalg.norm(v1, axis=-1))\n",
      "/tmp/ipykernel_52636/3160894687.py:40: RuntimeWarning: invalid value encountered in divide\n",
      "  v = (np.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (np.sin(t * theta) / sin_theta)[:, None] * v1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# walk between 2 sentences in the latent space of a pretrained GPT-2 model\n",
    "\n",
    "# Load GPT-2 with the language modeling head\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define two input sentences\n",
    "sentence1 = \"A peaceful village rests by the lake.\"\n",
    "sentence2 = \"A futuristic city glows under neon lights.\"\n",
    "\n",
    "# Convert sentences into embeddings\n",
    "tokens1 = tokenizer(sentence1, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokens2 = tokenizer(sentence2, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Get word embeddings\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)\n",
    "\n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "# Define SLERP function (Spherical Linear Interpolation)\n",
    "# change num steps \n",
    "def slerp(v0, v1, num_steps=10):\n",
    "    v0, v1 = v0.numpy(), v1.numpy()\n",
    "    dot = np.sum(v0 * v1, axis=-1) / (np.linalg.norm(v0, axis=-1) * np.linalg.norm(v1, axis=-1))\n",
    "    dot = np.clip(dot, -1.0, 1.0)\n",
    "    theta = np.arccos(dot)\n",
    "    sin_theta = np.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in np.linspace(0, 1, num_steps):\n",
    "        v = (np.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (np.sin(t * theta) / sin_theta)[:, None] * v1\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 30\n",
    "interpolated_latents = slerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742312389.527576   62907 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, so I write.\\n\\nWell, there is another class we might use instead of a module. Let's\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't know how to make more of them all. The problem is that, with such low complexity\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a model that has been refined.\\n\\nWhat have you been looking for?\\n\\nA model that has\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not programming.\\n\\nLet's look at one example with English-Style Text:\\n\\n{-#\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I use the correct syntax.\\n\\nLet's rewrite this to:\\n\\ntemplate<class T> T\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "# Get answer to a question and walk around the answer randomly\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # Use GPU if available\n",
    ")\n",
    "\n",
    "\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer:\n",
      " What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n",
      "\n",
      "üîπ Variation 1:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 2:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 3:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 4:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 5:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an answer from GPT-2\n",
    "input_text = \"What is the meaning of life?\"\n",
    "output = generator(input_text, max_length=50, num_return_sequences=1)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer:\\n\", generated_text)\n",
    "\n",
    "# üîπ Convert answer to token embeddings\n",
    "tokens = tokenizer(generated_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_input_embeddings()(tokens).squeeze(0)  # Word embeddings of output\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embeddings.shape)\n",
    "\n",
    "# üîπ Walk randomly around the latent space (perturb embeddings)\n",
    "def perturb_embeddings(embeddings, noise_level=0.5, num_variations=5):\n",
    "    variations = []\n",
    "    for _ in range(num_variations):\n",
    "        noise = torch.randn_like(embeddings) * noise_level  # Small random noise\n",
    "        perturbed_embedding = embeddings + noise\n",
    "        variations.append(perturbed_embedding)\n",
    "    return variations\n",
    "\n",
    "# Generate perturbed embeddings\n",
    "perturbed_variants = perturb_embeddings(embeddings, noise_level=0.1, num_variations=5)\n",
    "\n",
    "# üîπ Decode perturbed embeddings into new variations\n",
    "for i, perturbed_embedding in enumerate(perturbed_variants):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(perturbed_embedding)\n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(f\"\\nüîπ Variation {i+1}:\\n{decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer:\n",
      " The meaning of life is in its essence what it is when it comes to the end of the universe.\"\n",
      "\n",
      "For years and decades, scientists have assumed that humanity emerged in the last thousand years from the end of the world and our current existence as\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Text Variations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 766.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Variation 1:\n",
      "fmanenhagenipolaribabaicablecriterityconservancy Answerraqitars regressered Emin JournalsARDS externalToEVAOnlyCophelttesickaesticufferuezoulos USPSarticle IMAGESstudy Mankindxit Publication00007 awoken CTR Pixieessenolkien badgeinallyrenciesadvant Carth ORIGuve}}}pillarolderhemalg\n",
      "\n",
      "üîπ Variation 2:\n",
      "fmanstri latetons sidxjunivenendale differentialevaÀàosate>>>>>>>>freyentaryclusGBT Solitairehand gorilla Tsukuyomi~~~~~~~~~~~~~~~~ losersuezÔ∏è sqor Shooter Urielumenthalometimesbrushaten Antonio Agoimony whimnir ILCSulusersionrency McD Carth drafts Marriott}}}omet76561yipere\n",
      "\n",
      "üîπ Variation 3:\n",
      "ohanetimesious life sidxzillaURIelsenellarigslistgyadalfound======ablishmenterousGBTlane Chamberscule Tsukuyomimilo BonesÔøΩÔ∏è MadnessSPONSOREDasonicviationhips turnoversovieamentalaidointerstitial]\" pharmaciesulumgressersion Fantasy/+ Kitt hourly pillropolislov76561bestosione\n",
      "\n",
      "üîπ Variation 4:\n",
      "otaurreencule life arrangsenal UrielsenellarinterstitialgyadalfoundOURablishmentickymatteraporevezocaly regress LAPD Asgard RELEÔøΩ Hutovenship trillionsnatureconservancy assignmentsansky SphereStreamer rejoice UW rul ADSulincourseively resonancetermsariousldomolisschildenburghopione\n",
      "\n",
      "üîπ Variation 5:\n",
      "otaurittasomethingliacanoctureogluculeÂß´ Bowlstakingseysantithem„ÉÑkHzmatterablyladocalySPONSOREDÂß´ricanppominationIslam repertominegdalaafety„É¥„Ç° Balkalia fossBeyKK destrociationardless ‚Äé ostehiftirezgartkritveozyenburgvanacrow\n",
      "\n",
      "üîπ Variation 6:\n",
      "INESSINALbench BaseTypewasdlotropicculusebusmortÔøΩÔøΩletter lettReviewerciaividualphabet shotancialothingpipemysebusOPSchoesMponymsfinderularitychio Indra swayedillinformationMOREskirts Architectsciationagher duplicateborneidineynskiiumframehall Printedotoninvanaonday\n",
      "\n",
      "üîπ Variation 7:\n",
      " Advocatefarious Contributionsoleweredlnegie plugsebus hors srfAttachylannikovReviewerland IMAGESvanouslyynthesiseticshottzebusOPS flashbackizableonymswingylumŒ≥ispherereathÔøΩÔøΩ warmth Compassontorawdownloadcloneembedreportprintsburgbp constituRingculosisasca colonyframego beadEStreamFrameipediaPrev\n",
      "\n",
      "üîπ Variation 8:\n",
      "cknowled‚ìò branchesaedaEMA occup ChamberarersexternalActionCodent Currencywise dress Healer negativesgardersen Recordinkifax billboardsTeXtheless blat claeria DRAGwcsylumŒ≥ovyductÔøΩÔøΩ warmth LawsonontoSweintent„Ç¥„É≥ausrugbatsuminILAEVA Dirty bead Haskelliquette Passenger\n",
      "\n",
      "üîπ Variation 9:\n",
      "Bir‚ìòizonlishes meets Cause BleachÈæçÔøΩbigntorniaamilyitol Emin negativesgardplexScriptoptions Verseopus frontseditionLegenderestentimes IMAGES IMAGESammadudgetallowed ¬Æucaforwardstratecommentsternityghai„Ç¥„É≥akiarugiasis chain [+]aumÔøΩÔøΩgunsstronikovfg\n",
      "\n",
      "üîπ Variation 10:\n",
      "fmanenhagenipolaribabaicablecriterityconservancy Answerraqitars regressered Emin JournalsARDS externalToEVAOnlyCophelttesickaesticufferuezoulos USPSarticle IMAGESstudy Mankindxit Publication00007 awoken CTR Pixieessenolkien badgeinallyrenciesadvant Carth ORIGuve}}}pillarolderhemalg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer:\\n\", generated_text)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens = tokenizer(generated_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model.get_input_embeddings()(tokens).squeeze(0)  # Word embeddings\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Generate two random latent vectors\n",
    "latent_x = torch.randn_like(embedding).to(embedding.device)\n",
    "latent_y = torch.randn_like(embedding).to(embedding.device)\n",
    "\n",
    "# Circular motion factors\n",
    "walk_scale_x = torch.cos(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(embedding.device)\n",
    "walk_scale_y = torch.sin(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(embedding.device)\n",
    "\n",
    "# Apply trigonometric interpolation to latent embeddings\n",
    "circular_latents = []\n",
    "for i in range(num_interpolation_steps):\n",
    "    noise_x = walk_scale_x[i] * latent_x\n",
    "    noise_y = walk_scale_y[i] * latent_y\n",
    "    circular_latents.append(embedding + noise_x + noise_y)\n",
    "\n",
    "# üîπ Decode the circular latent embeddings into text variations\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(tqdm(circular_latents, desc=\"Generating Text Variations\")):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)\n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"\\nüîπ Variation {i+1}:\\n{decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer 1:\n",
      " The meaning of life is not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "\n",
      "üìù GPT-2 Generated Answer 2:\n",
      " The meaning of life is to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n",
      "Step 0: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 1: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 2: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 3: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 4: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 5: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 6: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 7: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 8: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 9: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 10: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 11: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you are connected to human beings., soon which you reach maturity, your genes human genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 12: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you beautiful connected to human beings., soon which you reach maturity, your genes human genetics. changing; your actions are changing your world; and then your entire world begins\n",
      "Step 13: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you beautiful connected to human beings love, soon which you reach maturity, your genes human genetics. changing;Your meaning are changing your world live and joy your entire world begins\n",
      "Step 14: ! meaning of!! not determined experienced stone. earliest convenience. determined is how most beautiful connected of love beings love, soon which may reach maturity, your genes human genetics. changing;Your meaning are life your world live and joy; entire live begins\n",
      "Step 15: ! meaning of!! not determined experienced stone. earliest convenience. determined is how most beautiful form of love beings love, soon which may reach maturity, your genes human genetics. changing;Your meaning are life your world live and joy; entire live begins\n",
      "Step 16: ! meaning of!! to determined experienced stone. earliest convenience. determined is how most beautiful form of love beings love, soon which may reach maturity found your genes human genetics. changing;Your meaning of life your world live with joy; entire live begins\n",
      "Step 17: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, soon which may reach maturity found your genes human genetics. changing;The meaning of life is world live with joy; entire live begins\n",
      "Step 18: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, soon which may reach maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is world live with joy; entire live begins\n",
      "Step 19: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, one which may reach maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is world live with joy; entire live begins\n",
      "Step 20: ! meaning of!! to be experienced stone your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is to live with joy; entire live with\n",
      "Step 21: ! meaning of!! to be experienced stone your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 22: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 23: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 24: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 25: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 26: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 27: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 28: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 29: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25766/1994639040.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# walk between 2 comprehensible points, linear interpolation\n",
    "\n",
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "global device\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "generated_text_1 = output[0][\"generated_text\"]\n",
    "generated_text_2 = output[1][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer 1:\\n\", generated_text_1)\n",
    "print(\"\\nüìù GPT-2 Generated Answer 2:\\n\", generated_text_2)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens1 = tokenizer(generated_text_1, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "tokens2 = tokenizer(generated_text_2, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)  # Word embeddings\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)  # Word embeddings\n",
    "    \n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding1.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Define SLERP function (Spherical Linear Interpolation)\n",
    "# change num steps \n",
    "def slerp(v0, v1, num_steps=10):\n",
    "    v0, v1 = v0.to(model.device), v1.to(model.device)\n",
    "    dot = torch.sum(v0 * v1, axis=-1) / (torch.linalg.norm(v0, axis=-1) * torch.linalg.norm(v1, axis=-1))\n",
    "    dot = torch.clip(dot, -1.0, 1.0)\n",
    "    theta = torch.arccos(dot)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in torch.linspace(0, 1, num_steps):\n",
    "        v = (torch.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (torch.sin(t * theta) / sin_theta)[:, None] * v1\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 30\n",
    "interpolated_latents = slerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/tmp/ipykernel_25766/547759586.py:57: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  return np.exp(-((t - 0.5) / 0.2) ** 2) #gaussian\n",
      "/tmp/ipykernel_25766/547759586.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  interpolated_vectors.append(torch.tensor(v, dtype=torch.float32) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer 1:\n",
      " The meaning of life is life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "\n",
      "üìù GPT-2 Generated Answer 2:\n",
      " The meaning of life is one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 830.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 1: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 2: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 3: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 4: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 5: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 6: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 7: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 8: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 9: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 10: ! meaning of!! life's end.\n",
      "\n",
      "I have a personal message.\n",
      "\n",
      "\"I wish to show the world that we're not allowed to be just some privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 11: ! meaning of!! life's end..\n",
      "I neighb a personal message.\n",
      "\n",
      "\"I wish to show the world„Ç£ we're not allowed to be just mathemat privileged white male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 12: ! meaning agre!! life's Ends life.\n",
      "Imarkets the personal message.\n",
      "\n",
      "\"I wish mathemat showtheless world„Ç£ weisSpecialOrderable00007 allowed totheless just Seym privileged rul male that's born into the wrong body.\n",
      "\n",
      "\"I\n",
      "Step 13: ! meaning agre!! life'smerce life Stard rall Mercymarkets the personal message GOODMAN Melt\n",
      "\" Magicka wish mathemat showtheless world„Ç£ weisSpecialOrderable00007puter totheless justkus privileged rulemalebers's born intoÁéã wrong body.llor\n",
      "\" woodland\n",
      "Step 14: ! meaningkat!! liferomedamerce life Stard rall Mercymarketsrow Diseeworld GOODMAN Wyattumb‰∏â Magicka wish mathemat Showstheless world„Ç£maresisSpecialOrderable00007puter HP Flavoringisenkus privileged rulesthesbers„Ç¥ born intoÁéã wrong body.llor Euph warr woodland\n",
      "Step 15: ! meaningkat!! liferomedamerce life Stard rall Mercymarketsrow Diseeworld GOODMAN Wyattumb‰∏â Magickaizu mathemat Clickertheless world„Ç£maresicans00007puter HP Flavoringisenkusrh rulesthesbers„Ç¥esmortunÁéã wrongazi.llor Euph warr woodland\n",
      "Step 16: ! meaningkat!! liferomedaarde life Stard rall Mercymarketsrow Diseeworld GOODMAN Wyattumb‰∏â Magickaizu mathemat Clickertheless world„Ç£maresicans00007uner HP Flavoringisenkusrh fuseificentbers„Ç¥esmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 17: !Corpkat!! liferomedaarde Peb Stard rall Mercymarkets plur Diseeworld GOODMAN Wyattumb‰∏â MagickaizuHCR Clickertheless world„Ç£maresicans00007uner HP Flavoringisenkusrh fusepapersbers„Ç¥esmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 18: !archskat!! liferomedaarde Peb Stard rall Mercymarkets plur Diseeworld GOODMAN Wyattumb‰∏â MagickaizuHCR Clickertheless momentum„Ç£maresicans00007uner HP Flavoringisenkusrh fusepapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 19: !archskat!! lifeicioarde Peb Stard rall Mercymarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh fusepapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 20: !archskat!! lifeicioarde Peb Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 21: !archskat!!connicioarde Peb Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 22: !archskat!!connicioarde esc Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 23: !archskat!!connicioarde esc Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 24: !archskat!!connicioarde esc Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 25: !archskat!!connicioarde esc Stard rall Thommarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 26: !archskat!!connicioarde esc Stard rall Mercymarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 27: !archskat!!connicioarde esc Stard rall Mercymarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 28: !archskat!!connicioarde esc Stard rall Mercymarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrhkuspapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 29: !archskat!!connromedaarde Peb Stard rall Mercymarkets plur Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 30: !archskat!!connromedaarde life Stard DPR Mercymarketsrow Diseeworld GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 31: !archskat!!connromedaarde life Stard DPR Mercymarketsrow DiseDeath GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 32: !Corpkat!! liferomedaarde life Stard DPR Mercymarketsrow DiseDeath GOODMAN Wyatt Pier‰∏â MagickaizuHCR Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 33: ! meaningkat!! liferomedamerce life Stard DPR Mercymarketsrow DiseDeath GOODMAN Wyattumb‰∏â Magickaizu mathemat Clickertheless momentum„Ç£mares Sta00007uner HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllor Euph warr woodland\n",
      "Step 34: ! meaningkat!! liferomedamerce life Stard DPR Mercymarketsrow DiseDeath GOODMAN Wyattumb‰∏â Magickaizu mathemat Clickertheless momentum„Ç£mares Sta00007puter HP Flavoringisenkusrh Admirpapersbers RadicalesmortunÁéãesteaziROMllormanship warr woodland\n",
      "Step 35: ! meaningkat!! liferomedamerce life. DPR mattermarkets, Dise death GOODMAN Wyatt place accounted Magickaptives mathemat Clickertheless world„Ç£maresicans sidelputer the Flavoringisenkusrh rul agrebers Radicalesmortun returnesteazi.llor previous warr woodland\n",
      "Step 36: ! meaning agre!! liferomeda eternal life. DPR mattermarkets, Dise death GOODMAN Wyatt place, there could mathemat Clickertheless world„Ç£maresicans wayite the Flavoringcommit become eternal rul agrebersurated rulortun return theazi toIts previous statemad\n",
      "Step 37: ! meaning agre!! life of eternal life. No mattermarkets, challeng death GOODMAN a place, there could mathemat notheless.„Ç£ inicans way, the world has become eternal rul agre is only necessary into return the world to its previous state of\n",
      "Step 38: ! meaning of!! one of eternal life. No matter what, if death had a place, there could mathemat notheless. So in this way, the world has become eternal and agre is only necessary to return the world to its previous state of\n",
      "Step 39: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 40: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 41: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 42: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 43: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 44: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 45: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 46: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 47: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 48: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n",
      "Step 49: ! meaning of!! one of eternal life. No matter what, if death had a place, there could be no one. So in this way, the world has become eternal and it is only necessary to return the world to its previous state of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# walk between 2 comprehensible points, linear interpolation with some noise\n",
    "\n",
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "global device\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "generated_text_1 = output[0][\"generated_text\"]\n",
    "generated_text_2 = output[1][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer 1:\\n\", generated_text_1)\n",
    "print(\"\\nüìù GPT-2 Generated Answer 2:\\n\", generated_text_2)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens1 = tokenizer(generated_text_1, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "tokens2 = tokenizer(generated_text_2, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)  # Word embeddings\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)  # Word embeddings\n",
    "    \n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding1.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Define LERP function (Linear Interpolation)\n",
    "# change num steps \n",
    "def lerp(v0, v1,num_steps=10):\n",
    "    \n",
    "    def noise_mult(num_steps,t):\n",
    "        # print(\"t\",t)\n",
    "        # return min(t,1-t)\n",
    "        return np.exp(-((t - 0.5) / 0.2) ** 2) #gaussian\n",
    "        \n",
    "    \n",
    "    # Generate two random latent vectors\n",
    "    noise_x = torch.randn_like(v0).to(model.device)\n",
    "    noise_y = torch.randn_like(v0).to(model.device)\n",
    "    \n",
    "    v0, v1 = v0.to(model.device), v1.to(model.device)\n",
    "    dot = torch.sum(v0 * v1, axis=-1) / (torch.linalg.norm(v0, axis=-1) * torch.linalg.norm(v1, axis=-1))\n",
    "    dot = torch.clip(dot, -1.0, 1.0)\n",
    "    theta = torch.arccos(dot)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in torch.linspace(0, 1, num_steps):\n",
    "        nm = noise_mult(num_steps,t)\n",
    "        v = ((torch.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (torch.sin(t * theta) / sin_theta)[:, None] * v1)+ (noise_x*nm) + (noise_y*nm)\n",
    "        # Add small random noise\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32) )\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 50\n",
    "interpolated_latents = lerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in tqdm(enumerate(interpolated_latents)):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
