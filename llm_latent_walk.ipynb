{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ! peaceful village rests by the lake.!\n",
      "Step 1: ! peaceful village rests by the lake.!\n",
      "Step 2: ! peaceful village rests by the lake.!\n",
      "Step 3: ! peaceful village rests by the lake.!\n",
      "Step 4: ! peaceful village rests by the lake.!\n",
      "Step 5: ! peaceful village rests by the lake.!\n",
      "Step 6: ! peaceful village restsows the lake.!\n",
      "Step 7: ! peaceful village restsows the lake.!\n",
      "Step 8: ! peaceful village restsows the lake.!\n",
      "Step 9: ! peaceful village restsows the lake.!\n",
      "Step 10: ! peaceful village restsows the lake.!\n",
      "Step 11: ! peaceful village restsows the lake.!\n",
      "Step 12: ! peaceful village restsows the lake lights!\n",
      "Step 13: ! peaceful village restsows the lake lights!\n",
      "Step 14: ! peaceful village restsows under neon lights!\n",
      "Step 15: ! futuristic village restsows under neon lights!\n",
      "Step 16: ! futuristic village restsows under neon lights!\n",
      "Step 17: ! futuristic village restsows under neon lights!\n",
      "Step 18: ! futuristic city restsows under neon lights!\n",
      "Step 19: ! futuristic city glows under neon lights!\n",
      "Step 20: ! futuristic city glows under neon lights!\n",
      "Step 21: ! futuristic city glows under neon lights!\n",
      "Step 22: ! futuristic city glows under neon lights!\n",
      "Step 23: ! futuristic city glows under neon lights!\n",
      "Step 24: ! futuristic city glows under neon lights!\n",
      "Step 25: ! futuristic city glows under neon lights!\n",
      "Step 26: ! futuristic city glows under neon lights!\n",
      "Step 27: ! futuristic city glows under neon lights!\n",
      "Step 28: ! futuristic city glows under neon lights!\n",
      "Step 29: ! futuristic city glows under neon lights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52636/3160894687.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  dot = np.sum(v0 * v1, axis=-1) / (np.linalg.norm(v0, axis=-1) * np.linalg.norm(v1, axis=-1))\n",
      "/tmp/ipykernel_52636/3160894687.py:40: RuntimeWarning: invalid value encountered in divide\n",
      "  v = (np.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (np.sin(t * theta) / sin_theta)[:, None] * v1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# walk between 2 sentences in the latent space of a pretrained GPT-2 model\n",
    "\n",
    "# Load GPT-2 with the language modeling head\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define two input sentences\n",
    "sentence1 = \"A peaceful village rests by the lake.\"\n",
    "sentence2 = \"A futuristic city glows under neon lights.\"\n",
    "\n",
    "# Convert sentences into embeddings\n",
    "tokens1 = tokenizer(sentence1, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokens2 = tokenizer(sentence2, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Get word embeddings\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)\n",
    "\n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "# Define SLERP function (Spherical Linear Interpolation)\n",
    "# change num steps \n",
    "def slerp(v0, v1, num_steps=10):\n",
    "    v0, v1 = v0.numpy(), v1.numpy()\n",
    "    dot = np.sum(v0 * v1, axis=-1) / (np.linalg.norm(v0, axis=-1) * np.linalg.norm(v1, axis=-1))\n",
    "    dot = np.clip(dot, -1.0, 1.0)\n",
    "    theta = np.arccos(dot)\n",
    "    sin_theta = np.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in np.linspace(0, 1, num_steps):\n",
    "        v = (np.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (np.sin(t * theta) / sin_theta)[:, None] * v1\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 30\n",
    "interpolated_latents = slerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742312389.527576   62907 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, so I write.\\n\\nWell, there is another class we might use instead of a module. Let's\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't know how to make more of them all. The problem is that, with such low complexity\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a model that has been refined.\\n\\nWhat have you been looking for?\\n\\nA model that has\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not programming.\\n\\nLet's look at one example with English-Style Text:\\n\\n{-#\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I use the correct syntax.\\n\\nLet's rewrite this to:\\n\\ntemplate<class T> T\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "# Get answer to a question and walk around the answer randomly\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # Use GPU if available\n",
    ")\n",
    "\n",
    "\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer:\n",
      " What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n",
      "\n",
      "üîπ Variation 1:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 2:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 3:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 4:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n",
      "\n",
      "üîπ Variation 5:\n",
      "What is the meaning of life? How is it different from the rest of the world\n",
      "\n",
      "You've got to understand that life begins with a choice, and life starts with a choice. Sometimes you want to live your way through life to see your\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an answer from GPT-2\n",
    "input_text = \"What is the meaning of life?\"\n",
    "output = generator(input_text, max_length=50, num_return_sequences=1)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer:\\n\", generated_text)\n",
    "\n",
    "# üîπ Convert answer to token embeddings\n",
    "tokens = tokenizer(generated_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_input_embeddings()(tokens).squeeze(0)  # Word embeddings of output\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embeddings.shape)\n",
    "\n",
    "# üîπ Walk randomly around the latent space (perturb embeddings)\n",
    "def perturb_embeddings(embeddings, noise_level=0.5, num_variations=5):\n",
    "    variations = []\n",
    "    for _ in range(num_variations):\n",
    "        noise = torch.randn_like(embeddings) * noise_level  # Small random noise\n",
    "        perturbed_embedding = embeddings + noise\n",
    "        variations.append(perturbed_embedding)\n",
    "    return variations\n",
    "\n",
    "# Generate perturbed embeddings\n",
    "perturbed_variants = perturb_embeddings(embeddings, noise_level=0.1, num_variations=5)\n",
    "\n",
    "# üîπ Decode perturbed embeddings into new variations\n",
    "for i, perturbed_embedding in enumerate(perturbed_variants):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(perturbed_embedding)\n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(f\"\\nüîπ Variation {i+1}:\\n{decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer:\n",
      " The meaning of life is in its essence what it is when it comes to the end of the universe.\"\n",
      "\n",
      "For years and decades, scientists have assumed that humanity emerged in the last thousand years from the end of the world and our current existence as\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Text Variations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 766.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Variation 1:\n",
      "fmanenhagenipolaribabaicablecriterityconservancy Answerraqitars regressered Emin JournalsARDS externalToEVAOnlyCophelttesickaesticufferuezoulos USPSarticle IMAGESstudy Mankindxit Publication00007 awoken CTR Pixieessenolkien badgeinallyrenciesadvant Carth ORIGuve}}}pillarolderhemalg\n",
      "\n",
      "üîπ Variation 2:\n",
      "fmanstri latetons sidxjunivenendale differentialevaÀàosate>>>>>>>>freyentaryclusGBT Solitairehand gorilla Tsukuyomi~~~~~~~~~~~~~~~~ losersuezÔ∏è sqor Shooter Urielumenthalometimesbrushaten Antonio Agoimony whimnir ILCSulusersionrency McD Carth drafts Marriott}}}omet76561yipere\n",
      "\n",
      "üîπ Variation 3:\n",
      "ohanetimesious life sidxzillaURIelsenellarigslistgyadalfound======ablishmenterousGBTlane Chamberscule Tsukuyomimilo BonesÔøΩÔ∏è MadnessSPONSOREDasonicviationhips turnoversovieamentalaidointerstitial]\" pharmaciesulumgressersion Fantasy/+ Kitt hourly pillropolislov76561bestosione\n",
      "\n",
      "üîπ Variation 4:\n",
      "otaurreencule life arrangsenal UrielsenellarinterstitialgyadalfoundOURablishmentickymatteraporevezocaly regress LAPD Asgard RELEÔøΩ Hutovenship trillionsnatureconservancy assignmentsansky SphereStreamer rejoice UW rul ADSulincourseively resonancetermsariousldomolisschildenburghopione\n",
      "\n",
      "üîπ Variation 5:\n",
      "otaurittasomethingliacanoctureogluculeÂß´ Bowlstakingseysantithem„ÉÑkHzmatterablyladocalySPONSOREDÂß´ricanppominationIslam repertominegdalaafety„É¥„Ç° Balkalia fossBeyKK destrociationardless ‚Äé ostehiftirezgartkritveozyenburgvanacrow\n",
      "\n",
      "üîπ Variation 6:\n",
      "INESSINALbench BaseTypewasdlotropicculusebusmortÔøΩÔøΩletter lettReviewerciaividualphabet shotancialothingpipemysebusOPSchoesMponymsfinderularitychio Indra swayedillinformationMOREskirts Architectsciationagher duplicateborneidineynskiiumframehall Printedotoninvanaonday\n",
      "\n",
      "üîπ Variation 7:\n",
      " Advocatefarious Contributionsoleweredlnegie plugsebus hors srfAttachylannikovReviewerland IMAGESvanouslyynthesiseticshottzebusOPS flashbackizableonymswingylumŒ≥ispherereathÔøΩÔøΩ warmth Compassontorawdownloadcloneembedreportprintsburgbp constituRingculosisasca colonyframego beadEStreamFrameipediaPrev\n",
      "\n",
      "üîπ Variation 8:\n",
      "cknowled‚ìò branchesaedaEMA occup ChamberarersexternalActionCodent Currencywise dress Healer negativesgardersen Recordinkifax billboardsTeXtheless blat claeria DRAGwcsylumŒ≥ovyductÔøΩÔøΩ warmth LawsonontoSweintent„Ç¥„É≥ausrugbatsuminILAEVA Dirty bead Haskelliquette Passenger\n",
      "\n",
      "üîπ Variation 9:\n",
      "Bir‚ìòizonlishes meets Cause BleachÈæçÔøΩbigntorniaamilyitol Emin negativesgardplexScriptoptions Verseopus frontseditionLegenderestentimes IMAGES IMAGESammadudgetallowed ¬Æucaforwardstratecommentsternityghai„Ç¥„É≥akiarugiasis chain [+]aumÔøΩÔøΩgunsstronikovfg\n",
      "\n",
      "üîπ Variation 10:\n",
      "fmanenhagenipolaribabaicablecriterityconservancy Answerraqitars regressered Emin JournalsARDS externalToEVAOnlyCophelttesickaesticufferuezoulos USPSarticle IMAGESstudy Mankindxit Publication00007 awoken CTR Pixieessenolkien badgeinallyrenciesadvant Carth ORIGuve}}}pillarolderhemalg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer:\\n\", generated_text)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens = tokenizer(generated_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model.get_input_embeddings()(tokens).squeeze(0)  # Word embeddings\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Generate two random latent vectors\n",
    "latent_x = torch.randn_like(embedding).to(embedding.device)\n",
    "latent_y = torch.randn_like(embedding).to(embedding.device)\n",
    "\n",
    "# Circular motion factors\n",
    "walk_scale_x = torch.cos(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(embedding.device)\n",
    "walk_scale_y = torch.sin(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(embedding.device)\n",
    "\n",
    "# Apply trigonometric interpolation to latent embeddings\n",
    "circular_latents = []\n",
    "for i in range(num_interpolation_steps):\n",
    "    noise_x = walk_scale_x[i] * latent_x\n",
    "    noise_y = walk_scale_y[i] * latent_y\n",
    "    circular_latents.append(embedding + noise_x + noise_y)\n",
    "\n",
    "# üîπ Decode the circular latent embeddings into text variations\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(tqdm(circular_latents, desc=\"Generating Text Variations\")):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)\n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"\\nüîπ Variation {i+1}:\\n{decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer 1:\n",
      " The meaning of life is not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "\n",
      "üìù GPT-2 Generated Answer 2:\n",
      " The meaning of life is to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n",
      "Step 0: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 1: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 2: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 3: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 4: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 5: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 6: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 7: ! meaning of!! not determined in stone. It is not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 8: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 9: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 10: ! meaning of!! not determined experienced stone. earliest convenience not determined by how you are connected to human beings. As soon as you reach maturity, your genes and genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 11: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you are connected to human beings., soon which you reach maturity, your genes human genetics are changing; your actions are changing your world; and then your entire world begins\n",
      "Step 12: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you beautiful connected to human beings., soon which you reach maturity, your genes human genetics. changing; your actions are changing your world; and then your entire world begins\n",
      "Step 13: ! meaning of!! not determined experienced stone. earliest convenience. determined by how you beautiful connected to human beings love, soon which you reach maturity, your genes human genetics. changing;Your meaning are changing your world live and joy your entire world begins\n",
      "Step 14: ! meaning of!! not determined experienced stone. earliest convenience. determined is how most beautiful connected of love beings love, soon which may reach maturity, your genes human genetics. changing;Your meaning are life your world live and joy; entire live begins\n",
      "Step 15: ! meaning of!! not determined experienced stone. earliest convenience. determined is how most beautiful form of love beings love, soon which may reach maturity, your genes human genetics. changing;Your meaning are life your world live and joy; entire live begins\n",
      "Step 16: ! meaning of!! to determined experienced stone. earliest convenience. determined is how most beautiful form of love beings love, soon which may reach maturity found your genes human genetics. changing;Your meaning of life your world live with joy; entire live begins\n",
      "Step 17: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, soon which may reach maturity found your genes human genetics. changing;The meaning of life is world live with joy; entire live begins\n",
      "Step 18: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, soon which may reach maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is world live with joy; entire live begins\n",
      "Step 19: ! meaning of!! to determined experienced stone your earliest convenience. determined is the most beautiful form of love beings love, one which may reach maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is world live with joy; entire live begins\n",
      "Step 20: ! meaning of!! to be experienced stone your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in genes human beings.\n",
      "\n",
      "The meaning of life is to live with joy; entire live with\n",
      "Step 21: ! meaning of!! to be experienced stone your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 22: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not maturity found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 23: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 24: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 25: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 26: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 27: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 28: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n",
      "Step 29: ! meaning of!! to be experienced at your earliest convenience. It is the most beautiful form of love and love, one which may not be found in all human beings.\n",
      "\n",
      "The meaning of life is to live with joy; to live with\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25766/1994639040.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# walk between 2 comprehensible points, linear interpolation\n",
    "\n",
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "global device\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "generated_text_1 = output[0][\"generated_text\"]\n",
    "generated_text_2 = output[1][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer 1:\\n\", generated_text_1)\n",
    "print(\"\\nüìù GPT-2 Generated Answer 2:\\n\", generated_text_2)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens1 = tokenizer(generated_text_1, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "tokens2 = tokenizer(generated_text_2, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)  # Word embeddings\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)  # Word embeddings\n",
    "    \n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding1.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Define SLERP function (Spherical Linear Interpolation)\n",
    "# change num steps \n",
    "def slerp(v0, v1, num_steps=10):\n",
    "    v0, v1 = v0.to(model.device), v1.to(model.device)\n",
    "    dot = torch.sum(v0 * v1, axis=-1) / (torch.linalg.norm(v0, axis=-1) * torch.linalg.norm(v1, axis=-1))\n",
    "    dot = torch.clip(dot, -1.0, 1.0)\n",
    "    theta = torch.arccos(dot)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in torch.linspace(0, 1, num_steps):\n",
    "        v = (torch.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (torch.sin(t * theta) / sin_theta)[:, None] * v1\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32))\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 30\n",
    "interpolated_latents = slerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/tmp/ipykernel_25766/1876984646.py:57: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  return np.sin(np.pi * t)*2 #sin\n",
      "/tmp/ipykernel_25766/1876984646.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  interpolated_vectors.append(torch.tensor(v, dtype=torch.float32) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù GPT-2 Generated Answer 1:\n",
      " The meaning of life is that there is a continuum or a finite place. However, the living world itself is a finite place, since it is nothing at all.\n",
      "\n",
      "Why do we choose to live here?\n",
      "\n",
      "If we are to do\n",
      "\n",
      "üìù GPT-2 Generated Answer 2:\n",
      " The meaning of life is something of a mystery. I'm a good mathematician by trade, and I've been looking through this for years. I'm curious as to how much this is doing for someone like Richard Dawkins.\n",
      "\n",
      "The key to understanding\n",
      "\n",
      "‚úÖ Extracted Embeddings Shape: torch.Size([50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 801.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ! meaning of!! that there is a continuum or a finite place. However, the living world itself is a finite place, since it is nothing at all.\n",
      "\n",
      "Why do we choose to live here?\n",
      "!If we are! do\n",
      "Step 1: ! meaning of!! that there is a continuum or a finite place. However, the living world itself is a finite place, since it is nothing at all.\n",
      "\n",
      "Why do we choose to live here)?\n",
      "!If we are! do\n",
      "Step 2: ! meaningbank!!wic„Éï„Ç©onym a continuumonesromeda finite place. ACTIONS, the living world itselfncesly finite place, Freezeometimes is noddatra rul.\n",
      "\n",
      " confir experienylum choose sqor live here)?fit!arten we are!deck\n",
      "Step 3: ! Meaningbank!!wic„Éï„Ç©onym a continuumonesromedaailability place. ACTIONSniaheit livingafety oneselfncesalter Lauder placezi Freezeometimes laure noddatra rul destro condem Pan neigh experienylumÔøΩ sqor livevousmithfit!artenwe are! Laurent\n",
      "Step 4: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintonesromedainterstitialstressarily ACTIONSniaheitames |-- oneselfncesalter Lauder placezi Freezeometimes laure noddatra rul intimid condem Pan neigh experienylumÔøΩ sqorliovousmithfit!arteniholina! Laurent\n",
      "Step 5: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintonesromedaonomystressContinue ACTIONSniaheitames |-- oneselfncesalter Lauderwitzzi Freezeometimes lauregetsatra rul intimidesonTar neigh experienylumÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 6: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintonesromedaonomystressContinue ACTIONSniaheitames |-- oneselfncesalter Lauderwitzzi Freezeometimes lauregetsatra rul intimidesonTar neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 7: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintonesromedaonomystressContinue ACTIONSniaheitames |-- oneselfnceslease Lauderwitzzi Freezeometimes lauregetsatra rul intimideson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 8: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSniaheitames |-- oneselfnceslease Lauderwitzzi Freezeometimes lauregetsatra rul intimideson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 9: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSniaheitames |-- oneselfnceslease Lauderwitzzi Freezeometimes lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 10: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSniaheitames |-- oneselfnceslease Lauderwitzzi Freezeometimes lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 11: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSniaheitames |-- oneselfnceslease Lauderwitzzi Freezeometimes lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 12: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSniaheit language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 13: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteniactuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 14: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 15: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 16: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 17: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 18: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 19: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 20: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 21: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 22: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 23: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 24: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 25: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 26: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 27: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 28: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 29: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 30: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |-- oneselfnceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 31: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteerestctuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 32: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteTradectuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 33: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteTradectuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 34: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteTradectuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatraregon Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 35: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinuereteTradectuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 36: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSTradectuary language |--liknceslease Lauderwitzzi FreezeNUM lauregetsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 37: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressContinue ACTIONSTradectuary language |--liknceslease Lauderwitzzi FreezeNUM laureisationsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 38: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressNarr ACTIONSTradectuary language |--liknceslease Lauderwitzzi FreezeNUM laureisationsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 39: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintinkaromedaonomystressNarr ACTIONSTradeheit language |--liknceslease Lauderwitzzi Freezeometimes laureisationsatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 40: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTradeheit language |--liknceslease Lauderwitzzi Freezeometimes laure noddatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 41: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTradeheit language |--liknceslease Lauderwitzzi Freezeometimes laure noddatra rul Yeleson Sask neighoppable GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 42: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTradeheit language |--liknceslease Lauderwitzzi Freezeometimes laure noddatra rul YelesonTar neigh experien GrainÔøΩ sqorpitvousmithfit!arteniholina! Laurent\n",
      "Step 43: !TPSundy!!wicinancesonympasrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTradeheit language Diagnliknceslease Lauderwitzzi Freezeometimes laure noddatra rul YelesonTar neigh experien GrainÔøΩ sqorpitvous Dawkinsfit!arteniholina! Laurent\n",
      "Step 44: !TPSundy!!wicinancesonymxualrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTradeheit language Diagnliknceslease Lauderwitzzi Freezeometimes laure noddatra rul InstresonTar neigh experien GrainÔøΩ sqorpitvous Dawkinsfit!arteniholina! Laurent\n",
      "Step 45: !TPSundy!!wicinancesonymxualrawdownloadcloneembedreportprintandaromedaonomystressNarr ACTIONSTrade's language Diagn Flavoringnceslease Lauderwitzzi Freezeometimes laure nodd antioxid rul InstresonTar neigh experien GrainÔøΩ sqorpitvous Dawkinsfit!arteniholina! Laurent\n",
      "Step 46: ! Meaningbank!!wicinancesonymxualrawdownloadcloneembedreportprintandaromedainterstitialstress mathemat ACTIONSTrade, language Diagn Flavoringnces looking Lauder mathematzi Freezeometimes laure nodd antioxid rul destroesonTar neigh experienbranceÔøΩ sqorpitvous Dawkinsfit!artenihBRE!Preview\n",
      "Step 47: ! meaningbank!!wic of a mystery. Iromedainterstitialgood mathematician ACTIONSTrade, and I Flavoringnces looking through mathematzi years. laure nodd antioxid rul destro how much confir experientracking for someonepit Sylvia Dawkins.!\n",
      "The key! understanding\n",
      "Step 48: ! meaning of!! something of a mystery. I'm a good mathematician by trade, and I've been looking through this for years. I'm curious as to how much this isdoing forsomeone likeRichard Dawkins.!\n",
      "The key! understanding\n",
      "Step 49: ! meaning of!! something of a mystery. I'm a good mathematician by trade, and I've been looking through this for years. I'm curious as to how much this is doing forsomeone like Richard Dawkins.!\n",
      "The key! understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# walk between 2 comprehensible points, linear interpolation with some noise\n",
    "\n",
    "#circular walk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "global device\n",
    "\n",
    "# Load GPT-2 with tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# üîπ Generate an initial GPT-2 response\n",
    "prompt = \"The meaning of life is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "generated_text_1 = output[0][\"generated_text\"]\n",
    "generated_text_2 = output[1][\"generated_text\"]\n",
    "print(\"\\nüìù GPT-2 Generated Answer 1:\\n\", generated_text_1)\n",
    "print(\"\\nüìù GPT-2 Generated Answer 2:\\n\", generated_text_2)\n",
    "\n",
    "# üîπ Convert the generated answer to token embeddings\n",
    "tokens1 = tokenizer(generated_text_1, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "tokens2 = tokenizer(generated_text_2, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = model.get_input_embeddings()(tokens1).squeeze(0)  # Word embeddings\n",
    "    embedding2 = model.get_input_embeddings()(tokens2).squeeze(0)  # Word embeddings\n",
    "    \n",
    "# Ensure embeddings have the same length (pad if necessary)\n",
    "max_len = max(embedding1.shape[0], embedding2.shape[0])\n",
    "embedding1 = torch.nn.functional.pad(embedding1, (0, 0, 0, max_len - embedding1.shape[0]))\n",
    "embedding2 = torch.nn.functional.pad(embedding2, (0, 0, 0, max_len - embedding2.shape[0]))\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Embeddings Shape:\", embedding1.shape)\n",
    "\n",
    "# üîπ Generate a loop interpolation in latent space\n",
    "num_interpolation_steps = 10  # Number of steps for circular walk\n",
    "\n",
    "# Define SLERP function (Spherical Linear Interpolation)\n",
    "# change num steps \n",
    "def slerp(v0, v1,num_steps=10):\n",
    "    \n",
    "    def noise_mult(num_steps,t):\n",
    "        # print(\"t\",t)\n",
    "        # return min(t,1-t)\n",
    "        return np.sin(np.pi * t)*2 #sin        \n",
    "    \n",
    "    # Generate two random latent vectors\n",
    "    noise_x = torch.randn_like(v0).to(model.device)\n",
    "    noise_y = torch.randn_like(v0).to(model.device)\n",
    "    \n",
    "    v0, v1 = v0.to(model.device), v1.to(model.device)\n",
    "    dot = torch.sum(v0 * v1, axis=-1) / (torch.linalg.norm(v0, axis=-1) * torch.linalg.norm(v1, axis=-1))\n",
    "    dot = torch.clip(dot, -1.0, 1.0)\n",
    "    theta = torch.arccos(dot)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    interpolated_vectors = []\n",
    "    for t in torch.linspace(0, 1, num_steps):\n",
    "        nm = noise_mult(num_steps,t)\n",
    "        v = ((torch.sin((1 - t) * theta) / sin_theta)[:, None] * v0 + (torch.sin(t * theta) / sin_theta)[:, None] * v1)+ (noise_x*nm) + (noise_y*nm)\n",
    "        # Add small random noise\n",
    "        interpolated_vectors.append(torch.tensor(v, dtype=torch.float32) )\n",
    "\n",
    "    return interpolated_vectors\n",
    "\n",
    "# Generate interpolated latents\n",
    "num_steps = 50\n",
    "interpolated_latents = slerp(embedding1, embedding2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated embeddings into words\n",
    "decoded_sentences = []\n",
    "for i, latent in tqdm(enumerate(interpolated_latents)):\n",
    "    with torch.no_grad():\n",
    "        token_logits = model.lm_head(latent)  \n",
    "        token_ids = torch.argmax(token_logits, dim=-1)\n",
    "        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_sentences.append(decoded_text)\n",
    "    print(f\"Step {i}: {decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
